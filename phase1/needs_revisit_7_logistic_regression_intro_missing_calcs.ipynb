{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlS3r8_lPvRU"
   },
   "source": [
    "# Logistic Regression\n",
    "We now move on to a situation where instead of our label being a *real-valued* continuous number such as we had with the housing dataset,  our labels take on discrete single values.   We will start with the case where we have a single label y which takes on two values: 0 or 1.    We will typically use the following:\n",
    "* y=0 will correspond to the negative class, also sometimes called background\n",
    "* y=1 will correspond to the positive class, also sometimes called signal\n",
    "These labels of negative/positive or background/signal are of course arbitrary.  The choice of y=0 and y=1 for the two labels is *not*.\n",
    "\n",
    "Let's look at an example.  Earlier, we examined the pulsar dataset, which has 8 features and 1 class label.   We will make this problem simpler, and just focus on one feature: **Profile_skewness**.   If we plot the class label versus this feature we see the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1jG6ZiLjPnZI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import numpy as np\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "#\n",
    "# Read in all of the other digits\n",
    "fname = '/fs/scratch/PAS1585/HTRU2/HTRU_2a.csv'\n",
    "dfAll = pd.read_csv(fname)\n",
    "print(dfAll.head())\n",
    "#corr = dfAll.corr()\n",
    "#corr.style.background_gradient().set_precision(3)\n",
    "\n",
    "def enable_plotly_in_cell():\n",
    "  import IPython\n",
    "  from plotly.offline import init_notebook_mode\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "  '''))\n",
    "  init_notebook_mode(connected=False)\n",
    "  \n",
    "enable_plotly_in_cell()\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=dfAll['Profile_skewness'],\n",
    "    y=dfAll['class'],\n",
    "    mode='markers',\n",
    "    name=\"fitted data\"\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "layout = dict(\n",
    "    title='Comparing Pulsar Classes',\n",
    "    xaxis='Profile skewness',\n",
    "    yaxis='Class')\n",
    "iplot(dict(data=data))#,layout=layout),validate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emXfdDijSe-d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRuzmJuzUglq"
   },
   "source": [
    "# Logistic Regression and the definition of our hypothesis function\n",
    "Now we could use Linear Regression to fit this **y versus x** plot.   We would define our hypothesis or prediction function as before: $h_\\theta(X)=X\\theta$.  In this case we have a single feature, so we would have two $\\theta$ parameters: a slope and an intercept.   But notice that if we were to draw a straight line through the above data points, we would have predictions $h_\\theta(X)$ which would lie below y=0 as well as above y=1, whereas our data only has values at y=0 and at y=1.   \n",
    "\n",
    "By using **logistic regression** instead, we will have:  $0\\le h_\\theta(X) \\le 1$.\n",
    "\n",
    "To obtain this result, we will need to modify our previous hypothesis function.  Instead of $h(\\theta)=X\\theta$, we will use:\n",
    "$$h_\\theta(X)=g(X\\theta)$$\n",
    "where\n",
    "$$g(z)= {1\\over{1+e^{-z}}}$$\n",
    "This function is called the **sigmoid** or the **logistic** function.  Below we plot this function for various values of z from -15.0 to 15.0.\n",
    "\n",
    "Try extending the range to +/-50, or shortenting it to +/-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cenyA7SxZkNj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# evenly sampled time at 200ms intervals\n",
    "z = np.arange(-15.0, 15.0, 0.2)\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(z, 1.0/(1.0+np.exp(-z)), 'r--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-PLw5OADbP45"
   },
   "source": [
    "# The Interpretation of $h(\\theta)$\n",
    "Notice in the plot above we have the desired feature that our function tends to take on values of either 0 (when z is small) or 1 (when z is large).  If we go back to our definition of $h_\\theta(X)$:\n",
    "$$h_\\theta(X)=g(X\\theta)= {1\\over{1+e^{-X\\theta}}}$$\n",
    "and remembering that $h_\\theta(X)$ is our prediction of the class *y*, we see that our goal of logistic regression is to choose the parameters $\\theta$ so that our predictions $h_\\theta(X)$  are as close to our sample classes *y* as possible.\n",
    "\n",
    "Now since  $0\\le g(z) \\le 1$, this also means that  $0\\le h_\\theta(X)\\le 1$.   The class labels y can only take on the values 0 and 1, while  $h_\\theta(X)$ can be any number between 0 and 1.   How do we interpret  $h_\\theta(X)$?\n",
    "* We will say that for a given set of features for one sample,  $h_\\theta(X)$ is the probability that y=1 for that specific set of features.   So if  $h_\\theta(X)=0.9$ for some X, we will say that there is a 90% chance that y=1 for that X.  If instead  $h_\\theta(X)=0.05$ for some X, we will say that there is a 5% chance that y=1 for that X.\n",
    "* Formally, this means that  $h_\\theta(X)= P(y=1|x;\\theta)$\n",
    "* Since we have only two classes, we can also say that: $P(y=0|x;\\theta) = 1-P(y=1|x;\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S9dW6G3ffQ5"
   },
   "source": [
    "# The Decision Boundary\n",
    "The decision boundary is a way of drawing a line (or surface) between our feautures X that neatly separates what our model calls signal (y=1) and what it calls background (y=0).   The way we can draw this boundary is by noting that our model predicts probabilities, not classes.   But, we can easily convert these by deciding for instance:\n",
    "* if $h_\\theta(X)\\ge 0.5$ then our predicted class $y_{pred}=1$\n",
    "* if $h_\\theta(X)\\lt 0.5$ then our predicted class $y_{pred}=0$\n",
    "\n",
    "Our boundary between these two classes is then $h_\\theta(X)=0.5$.   Now, since:\n",
    "$$h_\\theta(X)=g(X\\theta)$$\n",
    "and we want\n",
    "$$g(z)=0.5$$ then when $$z=0$$\n",
    "this tells us that the decision boundary which best separates those paramaters X corresponding to $y_{pred}=1$ from those paramaters X corresponding to $y_{pred}=0$ can be found by setting $\\theta X=0$.\n",
    "\n",
    "We will use this fact in our example dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0it1mfhkMTY"
   },
   "source": [
    "# The Cost Function for Logistic Regression\n",
    "In the case of linear regression, we defined the cost function to be (using matrix notation):\n",
    " $$J(\\theta) = {1\\over{2m}}\\sum_{i=i}^m(h_\\theta(X^{(i)})  -y^{(i)})^2$$\n",
    " \n",
    "We are going to rewrite this as:\n",
    " $$J(\\theta) = {1\\over{m}}\\sum_{i=i}^m {1\\over{2}}Cost(h_\\theta(X^{(i)}),y^{(i)})$$\n",
    "\n",
    "where for linear regression, the Cost is:\n",
    "$$Cost(h_\\theta(X^{(i)}),y^{(i)}) = {1\\over{2}}(h_\\theta(X^{(i)})  -y^{(i)})^2$$\n",
    "\n",
    "How will we write the Cost function for logistic regression?   It turns out that we **can't** simply plug in the expression for $h_\\theta(X)$ that we derived above.  The formal reason is that the resulting function is **non-convex** and thus hard to minimize.  Instead, we will use the following expression for our Cost:\n",
    "\n",
    "$$Cost(h_\\theta(X^{(i)}),y^{(i)}) = -y\\log(h_\\theta(X)) - (1-y)\\log(1-h_\\theta(X))$$\n",
    "\n",
    "Note the following about this Cost:\n",
    "* If y=1, then Cost=-$\\log(h_\\theta(X))$\n",
    "* If y=0, then Cost=-$\\log(1-h_\\theta(X))$\n",
    "\n",
    "Remember that $h_\\theta(X)$ ranges from 0 to 1.   Given this, lets plot the cost for these two cases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyGLnX3Lt6M3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# evenly sampled time at 200ms intervals\n",
    "z = np.arange(0.001,0.9999,0.001)\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(z, -np.log(z), 'r^',z, -np.log(1-z), 'bs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kom5owj-u14A"
   },
   "source": [
    "Let's make sure we understand these two curves:\n",
    "* First, the x-axis is $h_\\theta(X)$, and it ranges from 0 to 1.\n",
    "* If y=1, we want our prediction $h_\\theta(X)$ to be close to 1.  In this case our Cost=-$\\log(h_\\theta(X))$, which corresponds to the red triangles in the plot above.  As $h_\\theta(X)$ gets closer to 1 (the correct class), the cost becomes small, and as $h_\\theta(X)$ gets closer to 0 (the incorrect class), the cost becomes becomes infinite.\n",
    "* If y=0, we want our prediciton $h_\\theta(X)$  to be close to 0.  In this case our Cost=-$\\log(1-h_\\theta(X))$, which corresponds to the blue squares in the plot above.  As $h_\\theta(X)$ gets closer to 0 (the correct class), the cost becomes small, and as $h_\\theta(X)$ gets closer to 1 (the incorrect class), the cost becomes becomes infinite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vE4sVEs4bY0t"
   },
   "source": [
    "## Implementing the sigmoid\n",
    "The sigmoid equation is:\n",
    "$$g(z) = {1 \\over{1+e^{-z}}}$$\n",
    "\n",
    "We need to remember that for our case, $z=X\\theta$.   $X$ is a m by (n+1) matrix, while $\\theta$ is a (n+1) by 1 matrix.   This means that z is a m by 1 matrix, and so g(z) will also be an m by 1 matrix.   So to implement g(z), we will need to use the numpy $exp$ function.   We implement this in the function below.\n",
    "\n",
    "We also implement the function to retirve the probability for a sample, give the feature vector X for that sample, as well as the fitted $\\theta$ values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSq7SmLz8xzf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "  sm = 1.0 / (1.0 + np.exp(z))\n",
    "  return sm\n",
    "\n",
    "def get_prob(Theta,Xp):\n",
    "  hTheta = sigmoid(-np.dot(Xp,Theta))\n",
    "  return hTheta.item(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRwq-lJ07unK"
   },
   "outputs": [],
   "source": [
    "\n",
    "Theta = np.array([1,2,3]).reshape(3,1)\n",
    "X = np.array([[1,1,1],[2,2,2]])\n",
    "print(\"Theta.shape\",Theta.shape)\n",
    "print(\"X.shape\",X.shape)\n",
    "z = -np.dot(X,Theta)\n",
    "print(z.shape,z)\n",
    "res = sigmoid(z)\n",
    "print(res.shape,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vj1ZMEY51TKc"
   },
   "source": [
    "# Implementing the Cost Function for Logistic Regression\n",
    "Our full cost function for logisitc regression looks like this:\n",
    "$$J(\\theta) = \\sum_{i=i}^m [-y\\log(h_\\theta(X)) - (1-y)\\log(1-h_\\theta(X))]$$\n",
    "\n",
    "Our goal is to find the values $\\theta$ which minimize $J(\\theta)$, given samples characterized by features vectors X and classification labels y.  Note that I dropped the (1/m) term since it is a constant.   This does not affect the minimization of J.\n",
    "\n",
    "Let's go over the terms in this function:\n",
    " 1.  We have **m samples**, with **n features** per sample.\n",
    " 2. $\\theta$: this is a matrix of dimension (n+1) by 1\n",
    " 2.  $X$: This is a matrix of dimension m by (n+1) (since we have m samples, and we prepend a column of 1's to the original *n* feature vectors).\n",
    " 3. $h_\\theta(X)$: This is a matrix of dimension m by 1.\n",
    " 4.  $y$: This is a matrix of dimension m by 1.\n",
    " \n",
    "Again, note that although I described each of the above as *matrices*, we will use numpy (2 dimensional) *arrays* to implement all of them.\n",
    " \n",
    " The template is listed below - add your code to make it work!\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5M7Yv261RrN"
   },
   "outputs": [],
   "source": [
    "def calc_cost_logistic(Theta,Xp,yp):\n",
    "#\n",
    "# Make sure you return the cost as a number and not a matrix\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oC1gh2uE5vZP"
   },
   "source": [
    "## Implementing Gradient Descent\n",
    "In order to implement gradient descent, we first need the gradient - the derivative of J with respect to our $\\theta$.\n",
    "\n",
    "Here is our cost function $J(\\theta)$:\n",
    " $$J(\\theta) = \\sum_{i=i}^m [-y\\log(h_\\theta(X)) - (1-y)\\log(1-h_\\theta(X))]$$\n",
    " \n",
    " And here is the derivative with respect to $\\theta$:\n",
    " $${\\delta J\\over \\delta \\theta_j} = \\sum_{i=i}^m(h_\\theta(X^{(i)})  -y^{(i)})\\cdot X^{(i)}$$\n",
    " \n",
    " Notice that this is exactly the same form as for linear regression - you should verify for yourself that this is the case! \n",
    " \n",
    " The template code is listed below - add your own code to make it work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRJuxDg7iOBa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_gradient_descent_logistic(Theta,Xp,yp):\n",
    "\n",
    "  \n",
    "  return delTheta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMcdoQMD6Bww"
   },
   "source": [
    "## Iterating until we converge\n",
    "The basic algorithm then to implement gradient descent looks like this:\n",
    "1. Initialize each of the $\\theta$ parameters to some reasonable value (0 is common, or a random number).\n",
    "2. Choose a learning rate $\\alpha$, maxmimum allowed iterations, and a precision for the cost decrease to reach.\n",
    "3. Have an outer loop that checks that we have not exceeded our maximum number of allowed iterations **AND** that the cost is still decreasing.\n",
    "4. Calculate the gradient and update our parameters like so:\n",
    "$$\\theta_j := \\theta_j - \\alpha {\\partial J\\over \\partial \\theta_j}(\\theta)$$\n",
    "5. Calculate the cost for this iteration and compare it to the cost of the previous iteration.\n",
    "6. If the change in cost is small enough (below our chosen precision), declare victory and jump out of the loop.\n",
    "\n",
    "It is helpful to keep track of the cost for each iteration, so you can plot it and inspect its behavior.   And of course you need to keep track of the last value of the $\\theta$ parameters so you can return them.\n",
    "\n",
    "You should fill in the template code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvDbsHmC6Icg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fit_data(Xp,yp,learningRate,max_iterations,scale=True,delta=0.001):\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "  return Theta,iterations,costList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rc9S8Q0I6ODH"
   },
   "source": [
    "## Preparing the data and running the algorithm\n",
    "We are now ready to apply our algorithm on a toy classiifcation dataset.   Thiis dataset models admissions into a university based on the the score of two exams.   \n",
    "* The features are 'score1' and 'score2'\n",
    "* The label is ''admitted\".  This is already in the form of 1 (admitted) and 0 (not admitted).\n",
    "\n",
    "Since our dataset has two features X1 and X2, and one label y.   We will need to combine X1 and X2 into a single 2D numpy array, and add a \"ones\" column (to the front of that array). \n",
    "\n",
    "However, before we add the ones column, it is **very helpful** to scale our input features.   This will **greatly** aid in converging.    We will use min-max scaling.   \n",
    "\n",
    "Keep in mind that if you use scaling:\n",
    "1.  If you want to predict labels using new (or old for that matter) features, you will have to scale those features using the **same** scaling parameters.\n",
    "2.  The parameters we get from minimizing our cost function are those using the scaled features, so the $\\theta$ values we get won't correspond to the model used to generate the data.  If you want **those** $\\theta$ values (and you probably will), you will need to apply a transform to obtain them.   This is shown below.\n",
    "\n",
    "The code below prepares the data, and then runs the fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPlRkG-x6cTP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scl = MinMaxScaler()\n",
    "\n",
    "#\n",
    "# Get the toy test/admissions dataset\n",
    "url = \"/fs/scratch/PAS1585/studentAcceptance/combined.csv\"\n",
    "df=pd.read_csv(url)\n",
    "XToFit = df[['score1','score2']].values\n",
    "yToFit = df[['admitted']].values\n",
    "\n",
    "#\n",
    "# Make sure feature data is normalized\n",
    "XToFit2 = scl.fit_transform(XToFit)\n",
    "#XToFit2 = XToFit\n",
    "#\n",
    "# Prepend the \"ones\" column\n",
    "ones = np.ones((len(XToFit2),1))\n",
    "XToFit2 = np.append(ones,XToFit2,axis=1)\n",
    "#\n",
    "# Make sure label data has the correct shape\n",
    "yToFit2 = yToFit.reshape(len(yToFit),1)\n",
    "#\n",
    "# Check shapes\n",
    "print(XToFit2.shape)\n",
    "print(yToFit2.shape)\n",
    "\n",
    "iterations_max = 10000\n",
    "learningRate = 0.001\n",
    "delta = 0.001\n",
    "Theta,iterations,costList = fit_data(XToFit2,yToFit2,learningRate,iterations_max,delta=delta)\n",
    "#Theta,costList = fit_data_minimize(XToFit,yToFit,learningRate,iterations)\n",
    "print(\"fit Theta \",Theta)\n",
    "print(\"iterations \",iterations)\n",
    "print(\"cost\",costList[-1])\n",
    "\n",
    "\n",
    "countTrue = 0\n",
    "foundTrue = 0\n",
    "m,features = XToFit2.shape\n",
    "#\n",
    "# Loop over our dataset again, and test each sample\n",
    "for Xtest,yTest in zip(XToFit2,yToFit2):\n",
    "#\n",
    "# Make sure the features and label from this sample have the correct shape\n",
    "  Xtest = Xtest.reshape(1,features)\n",
    "  yTest = yTest.reshape(1,1)\n",
    "#\n",
    "# Get the probability for this sample\n",
    "  thisProb = get_prob(Theta,Xtest)\n",
    "#\n",
    "# Keep track of correct classifications\n",
    "  if yTest.item(0)==1:\n",
    "    countTrue += 1\n",
    "  if thisProb>0.5:\n",
    "    foundTrue += 1\n",
    "    \n",
    "print(\"Total size \",len(yToFit),\"; actual true:\",countTrue,\"; found true: \",foundTrue)\n",
    "\n",
    "print(\"theta \",Theta)\n",
    "#print(\"cost \",costList[:-10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRH1FI8SEvj7"
   },
   "source": [
    "# Plot of the decision boundary\n",
    "The next two snippits allow you to plot the decision boundary.   The first does this in the scaled feature space.   The second plots the decision boundary in the unscaled (original) feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKBDAw2V62TB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_reg(X, y, beta): \n",
    "  y = y.reshape(len(y))\n",
    "  x_0 = X[np.where(y < 0.5)] \n",
    "  x_1 = X[np.where(y > 0.5)] \n",
    "  print(\"x_0\",x_0.shape,x_1.shape)\n",
    "  x1_max = np.amax(x_1)\n",
    "  x1_min = np.amin(x_1)\n",
    "  print(\"x1 max,min\",x1_max,x1_min)\n",
    "\t\n",
    "\t# plotting points with diff color for diff label \n",
    "  plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') \n",
    "  plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') \n",
    "\t\n",
    "\t# plotting decision boundary \n",
    "  x1 = np.arange(x1_min, x1_max, 0.1) \n",
    "  #x1 = np.arange(0, 1, 0.1) \n",
    "  x2 = -(beta[0,0] + beta[1,0]*x1)/beta[2,0] \n",
    "  # uncvomment these if you have 3 features\n",
    "  #x3 = np.arange(0, 1, 0.1) \n",
    "  #x2 = -(beta[0,0] + beta[1,0]*x1 + beta[3,0]*x3)/beta[2,0] \n",
    "  print(\"x1\",x1.shape,x2.shape)\n",
    "  plt.plot(x1, x2, c='k', label='reg line') \n",
    "\n",
    "  plt.xlabel('x1') \n",
    "  plt.ylabel('x2') \n",
    "  plt.legend() \n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgPV8Mce6w21"
   },
   "outputs": [],
   "source": [
    "\t\n",
    "def plot_reg_scale(X, y, beta, scl): \n",
    "  Xt = scl.inverse_transform(X[:,1:])\n",
    "  y = y.reshape(len(y))\n",
    "  x_0 = Xt[np.where(y < 0.5)] \n",
    "  x_1 = Xt[np.where(y > 0.5)] \n",
    "#\n",
    "# Inverse transform the points\n",
    "#  x_0 = scl.inverse_transform(x_0[:,1:])\n",
    "#  x_1 = scl.inverse_transform(x_1[:,1:])\n",
    "\t\n",
    "\t# plotting points with diff color for diff label \n",
    "\n",
    "  plt.scatter([x_0[:, 0]], [x_0[:, 1]], c='b', label='y = 0') \n",
    "  plt.scatter([x_1[:, 0]], [x_1[:, 1]], c='r', label='y = 1') \n",
    "\t\n",
    "\t# plotting decision boundary \n",
    "  x1 = np.linspace(0.0,1.0, 10) \n",
    "  x2 = -(beta[0,0] + beta[1,0]*x1)/beta[2,0] \n",
    "  #\n",
    "  # Uncomment if you have 3 features\n",
    "  #x3 = np.linspace(0.0,1.0, 10) \n",
    "  #x2 = -(beta[0,0] + beta[1,0]*x1 + beta[3,0]*x3)/beta[2,0] \n",
    "  print(\"x1\",x1.shape,x2.shape)\n",
    "  xline = np.append(x1.reshape(len(x1),1),x2.reshape(len(x2),1),axis=1)\n",
    "  # Uncomment if you have 3 features\n",
    "  #xline = np.append(xline,x2.reshape(len(x3),1),axis=1)\n",
    "  xline = scl.inverse_transform(xline)\n",
    "\n",
    "  plt.plot(xline[:,0], xline[:,1], c='k', label='reg line') \n",
    "\n",
    "  plt.xlabel('x1') \n",
    "  plt.ylabel('x2') \n",
    "  plt.legend() \n",
    "  plt.show() \n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HYmVxTi6jNu"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(XToFit2.shape)\n",
    "plot_reg(XToFit2, yToFit2, Theta)\n",
    "\n",
    "plot_reg_scale(XToFit2, yToFit2, Theta,scl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WMM4yPPtEpa4"
   },
   "source": [
    "# Plot of the cost vs iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SE33-zPM5XTL"
   },
   "outputs": [],
   "source": [
    "enable_plotly_in_cell()\n",
    "data = go.Scatter(\n",
    "    x=np.array(range(0,len(costList))),\n",
    "    y=costList,\n",
    "    mode='markers',\n",
    "    name=\"fitted data\"\n",
    ")\n",
    "\n",
    "\n",
    "iplot(dict(data=[data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXaGiJ7BXC1-"
   },
   "source": [
    "# Using a Different Minimization Technique\n",
    "In the work that we did above, we minimized our cost by \n",
    "1.  computing the gradient of our cost function\n",
    "2.  picking a learning rate $\\alpha$ and maximum number of iterations\n",
    "3.  looping until we found that the change in our cost fell below some threshold or we exceeded the number of iterations\n",
    "\n",
    "This technique worked well.   However, we are not given much guidance in how to choose the learning rate, and in fact if you use the logistic regression technique on a range of different problems, you will find that you will need to adjust $\\alpha$ in order to make sure that the cost is indeed minimized.  \n",
    "\n",
    "There is another way to minimize our cost function, and it involves using advanced algorithms which are dedicated to doing just that.  One such package which provides these algrotihms is called **scipy**.   These packages typically require the following:\n",
    "1.  A \"callable\" function which calculates the thing you are trying to minimize: for us this is **calc_cost_logistic**.\n",
    "2.  An optional \"callable\" function which calculates the gradient of the thing you are trying to minimize: for us this **should be calc_gradient_descent_logistic**.   Unfortunately due to how scipy implements it minimization we need to modify our function slightly.\n",
    "\n",
    "The minimization method we will use is **scipy.optimize.minimize**.   The calling format is one of these two:\n",
    "* result = minimize(fun, x0, args=(), method='BFGS')\n",
    "* result = minimize(fun, x0, args=(), method='BFGS', jac=xxx)\n",
    "\n",
    "The terms in the above are the following:\n",
    "1.  **fun**: this is the function to minimize, which in our case is **calc_cost_logistic**.   It is assumed that the \"parameters\" that are being modified and returned by **minimize** are the **first argument** to calc_cost_logistic.   For us this is the **Theta** array.\n",
    "2. **xo**: this is the initial values of the parameters you are minimizing (again this is **theta**).\n",
    "3. **args=()**:  This is an optional argument, but it will be a *tuple* of all of the other arguments to your **fun**.   For us we call **calc_cost_logistic** like this:\n",
    "      *     cost = calc_cost_logistic(Theta,XToFit2,yToFit2)\n",
    "so **args=(XToFit2,yToFit2)**.\n",
    "4.**method**: This is the optimzation method.  Scipy includes a large number of possibilities - see the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) for more info.   For now, we will use 'BFGS' (or Broyden–Fletcher–Goldfarb–Shanno) algorithm.  \n",
    "5. **jac**: This supplies the gradient of the cost function with respect to the parameters.   Normally this would just be **calc_gradient_descent_logistic** but due to a slight issue with how we implemented it, this willl not work without a minor modification.   This is described below suing the function **calc_gradient_descent_logistic_new**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxLgiXS3jGhU"
   },
   "outputs": [],
   "source": [
    "def calc_gradient_descent_logistic_new(Theta,Xp,yp):\n",
    "# Your code here:\n",
    "  \n",
    "  #\n",
    "  # scipy expects a returned result with dimensions (len(Theta),) instead of (len(Theta),1)\n",
    "  delTheta = np.ndarray.flatten(delTheta)\n",
    "  return delTheta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRcvepVRXrvk"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "Theta_0 = np.random.randn(features,1)\n",
    "Theta_0 = np.zeros((features,1))\n",
    "\n",
    "\n",
    "result = minimize(calc_cost_logistic, Theta_0, args=(XToFit2,yToFit2), method = 'BFGS')\n",
    "print()\n",
    "print(\"result\",result)\n",
    "print(\"minimized Theta\",result['x'])\n",
    "\n",
    "result = minimize(calc_cost_logistic, Theta_0, jac=calc_gradient_descent_logistic_new,args=(XToFit2,yToFit2), method = 'BFGS')\n",
    "print()\n",
    "print(\"result\",result)\n",
    "print(\"minimized Theta\",result['x'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the decision boundary\n",
    "Now let's plot the decision boundary using the results of the above fit.   Notice that the fitted $\\theta$ results are in the returned **result** dictionary, with key \"x\".  \n",
    "\n",
    "We will have to reshape the resulting $\\theta$ since our plotting routine expects an array of shape (3,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Theta.shape)\n",
    "ThetaNew = result['x']\n",
    "print(\"old shape \",ThetaNew.shape)\n",
    "ThetaNew = ThetaNew.reshape(3,1)\n",
    "print(\"new shape \",ThetaNew.shape)\n",
    "\n",
    "plot_reg(XToFit2, yToFit2, ThetaNew)\n",
    "\n",
    "plot_reg_scale(XToFit2, yToFit2, ThetaNew,scl)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "logistic_regression_intro_missing_calcs.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
